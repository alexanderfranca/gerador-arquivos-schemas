{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_snake(name):\n",
    "    name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snake_to_camel(word):\n",
    "    import re\n",
    "    return ''.join(x.capitalize() or '_' for x in word.split('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria os diretorios para onde vao os arquivos gerados\n",
    "dirs = ['json', 'ddl', 'dags', 'datalake', 'sql', 'variables']\n",
    "\n",
    "pathlib.Path('./enviar').mkdir(parents=True, exist_ok=True)\n",
    "for directory in dirs:\n",
    "    pathlib.Path(f\"./enviar/{directory}\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "##  BEGIN DEFINICOES MANUAIS OBRIGATORIAS ##\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome da conexao que vai para dentro da DAG\n",
    "conn_id = \"connection_name_id\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome do arquivo que possui os metadados da tabela.\n",
    "# IMPORTANTE: todos os nomes do cabecalho precisam estar em maiusculas.\n",
    "# Por exemplo: TABLE_NAME, DATA_TYPE etc\n",
    "# Toda vez que aparecer \"metadado\" escrito nos comentarios, se refere a esse arquivo e\n",
    "# seu conteudo.\n",
    "meta = pd.read_csv('metadados-example.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como o nome do schema esta no arquivo de metadados e ele nao possui a\n",
    "# convencao interna do time, se faz necessario criar uma estrutura de conversao\n",
    "# como abaixo.\n",
    "schemas_mapping = {\n",
    "    'example1': 'dump__company_co_ods__example1',\n",
    "    'example2': 'dump__company_co_ods__example2',\n",
    "    'correios': 'dump__company_co_ods__correios'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "##  END DEFINICOES MANUAIS OBRIGATORIAS ##\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isso aqui serve para o script adicionar ou nao aquela chave:\n",
    "# format: date_time\n",
    "# para os tipos de dados de data (que sao considerados como string dentro do arquivo).\n",
    "# Basicamente o script verifica se o data_type do metadado esta na lista abaixo,\n",
    "# se estiver, ele adicionar as linhas correpondentes para o \"format\": \"date-time\".\n",
    "tipos_data = ['datetime', 'datetime2', 'time', 'timestamp', 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrutura de conversao de tipos entre os metadados e o que se espera estar inserido\n",
    "# no json schema.\n",
    "data_type_conversion_source_json = {\n",
    "    'int': 'integer',\n",
    "    'datetime': 'string',\n",
    "    'datetime2': 'string',\n",
    "    'varchar': 'string',\n",
    "    'bit': 'boolean',\n",
    "    'char': 'string',\n",
    "    'smallint': 'integer',\n",
    "    'nvarchar': 'string',\n",
    "    'bigint': 'number',\n",
    "    'decimal': 'number',\n",
    "    'float': 'number',\n",
    "    'uniqueidentifier': 'string',\n",
    "    'time': 'string',\n",
    "    'date': 'string',\n",
    "    'real': 'number',\n",
    "    'varbinary': \"string\",\n",
    "    'text': 'string',\n",
    "    'numeric': 'number',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrutura de conversao entre os tipos de dados dos metadados e o arquivo DDL para \n",
    "# o bigquery.\n",
    "data_type_conversion_source_ddl = {\n",
    "    'int': 'INTEGER',\n",
    "    'datetime': 'TIMESTAMP',\n",
    "    'datetime2': 'TIMESTAMP',    \n",
    "    'varchar': 'STRING',\n",
    "    'bit': 'BOOLEAN',\n",
    "    'char': 'STRING',\n",
    "    'smallint': 'INTEGER',\n",
    "    'nvarchar': 'STRING',\n",
    "    'bigint': 'INTEGER',\n",
    "    'decimal': 'BIGNUMERIC',\n",
    "    'float': 'BIGNUMERIC',\n",
    "    'uniqueidentifier': 'STRING',\n",
    "    'time': 'TIMESTAMP',\n",
    "    'date': 'DATE',\n",
    "    'real': 'BIGNUMERIC',\n",
    "    'varbinary': \"STRING\",\n",
    "    'text': 'STRING',\n",
    "    'numeric': 'BIGNUMERIC',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abaixo eh o que gera a estrutura a partir dos metadados contendo todos os elementos\n",
    "# que vao ser utilizados para gerar os arquivos.\n",
    "raw_schema = {}\n",
    "\n",
    "for index, row in meta.iterrows():\n",
    "\n",
    "    if not row['TABLE_SCHEMA'] in raw_schema:\n",
    "        raw_schema[row['TABLE_SCHEMA']] = {}\n",
    "\n",
    "    column_snake = to_snake(row['COLUMN_NAME'])\n",
    "\n",
    "    if not row['TABLE_NAME'] in raw_schema[row['TABLE_SCHEMA']]:\n",
    "        raw_schema[row['TABLE_SCHEMA']][row['TABLE_NAME']] = []\n",
    "\n",
    "    raw_schema[row['TABLE_SCHEMA']][row['TABLE_NAME']].append({'column_snake': column_snake, 'column': row['COLUMN_NAME'], 'data_type': row['DATA_TYPE'], 'table': row['TABLE_NAME'], 'schema': row['TABLE_SCHEMA'], 'catalog': row['TABLE_CATALOG']})\n",
    "    tables.append(row['TABLE_NAME'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera efetivamente os arquivos\n",
    "# O Mako eh uma ferramenta de templates para Python.\n",
    "# Optei por ele e nao o Jinja porque o Jinja supoe um contexto de\n",
    "# aplicativo (coisas do Flask etc) e eu queria algo mais simplificado para usar.\n",
    "from mako.template import Template\n",
    "columns = []\n",
    "\n",
    "for schema in raw_schema:\n",
    "\n",
    "    tables = []\n",
    "    for table in raw_schema[schema]:\n",
    "        tables.append(table)\n",
    "        columns = []\n",
    "        \n",
    "        for d in raw_schema[schema][table]:\n",
    "\n",
    "            catalog = d['catalog']\n",
    "            column_snake = d['column_snake']\n",
    "            column = d['column']\n",
    "            table_snake = to_snake(table)\n",
    "            catalog_snake = to_snake(catalog)\n",
    "            schema_orig = schema\n",
    "            schema_translated = schemas_mapping[schema]\n",
    "            schema_snake = to_snake(schema)\n",
    "            data_type_json = data_type_conversion_source_json[d['data_type']]\n",
    "            data_type_ddl = data_type_conversion_source_ddl[d['data_type']]\n",
    "            \n",
    "            # Lembra daquela lista de tipos de dado datetime.\n",
    "            # Entao, eh aqui que ela eh consultada.\n",
    "            # Eh so para marcar se o script precisa adicionar a linha \"format\": \"date-time\"\n",
    "            # no json.\n",
    "            if d['data_type'] in tipos_data:\n",
    "                datetime = 1\n",
    "            else:\n",
    "                datetime = 0\n",
    "\n",
    "            columns.append({'table_sname': table_snake,\n",
    "                            'table': table, \n",
    "                            'schema_snake': schema_snake,\n",
    "                            'schema': schema,\n",
    "                            'catalog_snake': catalog_snake,\n",
    "                            'catalog': catalog,\n",
    "                            'schema_orig': schema_orig,\n",
    "                            'name_snake': column_snake, \n",
    "                            'name': column, \n",
    "                            'datetime': datetime, \n",
    "                            'data_type_ddl': data_type_ddl, \n",
    "                            'data_type_json': data_type_json})\n",
    "\n",
    "        # Aqui comecam a ser gerados os arquivos:\n",
    "        # ------------------------------------------\n",
    "        # Create Dags\n",
    "        \n",
    "        # Essas variaveis todas vao ser utilizadas dentro do template.\n",
    "        constant = f\"{schemas_mapping[schema].upper()}_{to_snake(table).upper()}\"\n",
    "        task_id = f\"{schemas_mapping[schema]}_{to_snake(table).lower()}\"\n",
    "        source_sql = f\"{schemas_mapping[schema]}/{to_snake(table).lower()}.sql\"\n",
    "        conn_id = conn_id\n",
    "        dag_id = f\"{schemas_mapping[schema]}_{to_snake(table).lower()}_extractor\"\n",
    "        max_active_runs = f\"{to_snake(table).lower()}\"\n",
    "        gcs_path = to_snake(table).lower()\n",
    "        \n",
    "        # O arquivo de template\n",
    "        mytemplate = Template(filename='./templates/dag.tpl')\n",
    "        \n",
    "        # Onde o resultado da geracao deve ser escrito\n",
    "        file_name = f\"./enviar/dags/{schemas_mapping[schema]}-{to_snake(table).lower()}_extractor.py\"\n",
    "        \n",
    "        # Repare que o arquivo eh escrito utilizando o render do Mako.\n",
    "        with open(file_name, 'w') as f:                    \n",
    "            f.write(mytemplate.render(constant=constant, task_id=task_id, source_sql = source_sql,\n",
    "                                     conn_id=conn_id, dag_id=dag_id, max_active_runs=max_active_runs,\n",
    "                                     schema=schemas_mapping[schema],\n",
    "                                     gcs_path=gcs_path))\n",
    "\n",
    "        # Segue o mesmo esquema acima, mas para os arquivos de schema json.\n",
    "        # Create schemas json\n",
    "        mytemplate = Template(filename='./templates/json.tpl')\n",
    "        file_name = f\"./enviar/json/{schemas_mapping[schema]}-{to_snake(table).lower()}.json\"\n",
    "        with open(file_name, 'w') as f:                    \n",
    "            f.write(mytemplate.render(catalog=catalog, schema_orig = schema_orig, schema_snake = schema_snake, schema=schema_translated, table_snake = table_snake, table=table, columns=columns))\n",
    "\n",
    "        # Create schemas ddl\n",
    "        mytemplate = Template(filename='./templates/ddl.tpl')            \n",
    "        file_name = f\"./enviar/ddl/{schemas_mapping[schema]}-{to_snake(table).lower()}.json\"\n",
    "        with open(file_name, 'w') as f:                    \n",
    "            f.write(mytemplate.render(catalog=catalog, schema_orig = schema_orig, schema_snake = schema_snake, schema=schema_translated, table_snake = table_snake, table=table, columns=columns))\n",
    "\n",
    "        # Create sql files\n",
    "        mytemplate = Template(filename='./templates/sql.tpl')            \n",
    "        file_name = f\"./enviar/sql/{to_snake(table).lower()}.sql\"\n",
    "        with open(file_name, 'w') as f:                    \n",
    "            f.write(mytemplate.render(catalog=catalog, schema_orig = schema_orig, schema_snake = schema_snake, schema=schema_translated, table_snake = table_snake, table=table, columns=columns))\n",
    "\n",
    "    # Aqui sai do loop aninhado porque eh sobre a lista de tabelas\n",
    "    # e nao a lista de colunas dentro das tabelas.\n",
    "    tmp_tables = []\n",
    "    for t in tables:\n",
    "        tmp_tables.append(to_snake(t))\n",
    "\n",
    "    tables = tmp_tables\n",
    "    del tmp_tables\n",
    "\n",
    "    # Create datalake.tf\n",
    "    mytemplate = Template(filename='./templates/datalake.tpl')\n",
    "    file_name = f\"./enviar/datalake/datalake.tf_{schemas_mapping[schema]}\"\n",
    "    with open(file_name, 'w') as f:                    \n",
    "        f.write(mytemplate.render(tables=tables))\n",
    "\n",
    "    # Create variables.json\n",
    "    mytemplate = Template(filename='./templates/variables.tpl')\n",
    "    file_name = f\"./enviar/variables/variables.json_{schemas_mapping[schema]}\"\n",
    "    with open(file_name, 'w') as f:                    \n",
    "        f.write(mytemplate.render(tables=tables, schema=schemas_mapping[schema]))              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
